什么是CNN的池化pool层？
取区域平均或最大

你有哪些deep learning(run, cnn)调参的经验？
a. 参数初始化
   Xavier初始法，适用于普通激活函数
   He初始化，适用于ReLU
   normal高斯分布初始化
   uniform均匀分布初始化
b. 数据预处理
   zero-center
c. 训练技巧
   clip c(梯度裁剪): 限制最大梯度
   dropout对小数据防止过拟合有很好的效果,值一般设为0.5
   尽量对数据做shuffle

CNN优点
存在局部与整体的关系，由低层次的特征经过组合，组成高层次的特征，并且得到不同特征之间的空间相关性

Sigmoid、Tanh、ReLu这三个激活函数有什么缺点或不足，有没改进的激活函数
Sigmoid:
       优点：输出在(0,1)之间，有明确的上下界。
       缺点：梯度消失问题， 输出非中心化
Tanh:
     优点：输出在(-1,1)之间，比sigmoid函数的输出范围更大，并且是以0为中心的。
     缺点：梯度消失问题
ReLU (Rectified Linear Unit)：
     优点：计算速度快，对于正的输入值，梯度为1，因此不受梯度消失问题的影响
     缺点： 死亡ReLU问题

为什么引入非线性激励函数？
1. 增加表示能力 如果不使用非线性激励函数，不论神经网络有多少层，它始终只能表示线性映射
2. 捕捉复杂关系 很多实际应用中的数据都包含复杂的非线性关系
3. 更深的模型 非线性激励函数使我们能够构建更深的神经网络，这在很大程度上增加了模型的容量和表示能力
4. 梯度下降优化

如何解决梯度消失和梯度膨胀？
（1）梯度消失：根据链式法则，如果每一层神经元对上一层的输出的偏导乘上权重结果都小于1的话，那么即使这个结果是0.99，在经过足够多层传播之后，误差对输入层的偏导会趋于0
可以采用ReLU激活函数有效的解决梯度消失的情况，也可以用Batch Normalization解决这个问题
（2）梯度爆炸：根据链式法则，如果每一层神经元对上一层的输出的偏导乘上权重结果都大于1的话，在经过足够多层传播之后，误差对输入层的偏导会趋于无穷大
     可以通过激活函数来解决，或用Batch Normalization解决这个问题

过拟合具体表现在：模型在训练数据上损失函数较小，预测准确率较高；但是在测试数据上损失函数比较大，预测准确率较低
正则化， 增大训练样本

L1 范数: 为 x 向量各个元素绝对值之和
L2 范数: 为 x 向量各个元素平方和的 1/2 次方，L2 范数又称 Euclidean 范数或 Frobenius 范数

批量归一化作用：
(1). 可以使用更高的学习率
（2）. 移除或使用较低的dropout。

O = (W-F+2P)/S+1
交叉熵刻画了两个概率分布之间的距离，是分类问题中使用比较广泛的一种损失函数
